{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.6\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "def _get_weaviate_client_version():\n",
    "    pkg_name = \"weaviate-client\"\n",
    "    try:\n",
    "        return version(pkg_name)\n",
    "    except PackageNotFoundError:\n",
    "        return None\n",
    "\n",
    "client_version = _get_weaviate_client_version()\n",
    "print(client_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import NaiveWcsQaRagAgent\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def display_md(content):\n",
    "  display(Markdown(content))\n",
    "\n",
    "doc_uri = \"https://arxiv.org/html/2312.10997v5\"\n",
    "qa_agent = NaiveWcsQaRagAgent(doc_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Naive RAG is the earliest methodology within the RAG research paradigm, emerging after the widespread adoption of ChatGPT. It follows a traditional process of indexing, retrieval, and generation, known as the \"Retrieve-Read\" framework. Naive RAG involves cleaning and extracting raw data in various formats, segmenting text into smaller chunks, encoding them into vector representations, and storing them in a vector database."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is naive RAG?\"\n",
    "completion = qa_agent.query(question)\n",
    "display_md(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The authors of the paper are Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who are the authors of this paper?\"\n",
    "completion = qa_agent.query(question)\n",
    "display_md(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Some benchmarks available for RAG include RGB, RECALL, and CRUD, which focus on evaluating essential abilities of RAG models. These benchmarks assess aspects like retrieval quality, generation quality, and counterfactual robustness. Additionally, metrics such as Hit Rate, MRR, and NDCG are commonly used to measure the performance of the RAG retrieval module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What are some benchmarks available for RAG?\"\n",
    "completion = qa_agent.query(question)\n",
    "display_md(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
